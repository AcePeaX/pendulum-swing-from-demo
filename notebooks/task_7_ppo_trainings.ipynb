{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276896fc",
   "metadata": {},
   "source": [
    "# PPO training for Task 7 pendulum\n",
    "\n",
    "Reproduces the PPO workflow from `task_2_lqr_balance_data_gen.ipynb`, but runs directly on the custom PyBullet + Pinocchio environment implemented in `scripts/task_7_env.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cc2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b9f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/acepeax/Desktop/Studies/MVA/Robotics/Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Dec  4 2025 20:11:42\n"
     ]
    }
   ],
   "source": [
    "current = Path.cwd()\n",
    "\n",
    "if (current / 'notebooks').exists():\n",
    "    PROJECT_ROOT = current\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "import sys\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from scripts.task_7_env import Task7PendulumEnv\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "MODELS_DIR = DATA_DIR\n",
    "for directory in (DATA_DIR, MODELS_DIR):\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad820117",
   "metadata": {},
   "source": [
    "## Environment helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8395dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (14,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 1000\n",
    "SHOULD_BALANCE = True\n",
    "\n",
    "def make_task7_env(gui=False, should_balance=SHOULD_BALANCE):\n",
    "    def _init():\n",
    "        env = Task7PendulumEnv(\n",
    "            max_steps=MAX_STEPS,\n",
    "            should_balance=should_balance,\n",
    "            gui=gui,\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "train_env = DummyVecEnv([make_task7_env(gui=False, should_balance=SHOULD_BALANCE)])\n",
    "print('Observation space:', train_env.observation_space)\n",
    "print('Action space:', train_env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c3c13",
   "metadata": {},
   "source": [
    "## Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dd7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_env,\n",
    "    verbose=0,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=3e-4,\n",
    "    clip_range=0.2,\n",
    "    tensorboard_log='runs/task7_ppo',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d885b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017ca58ae274422b014b0a627a7fcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m TOTAL_TIMESTEPS = \u001b[32m300_000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOTAL_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:227\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m policy_loss_1 = advantages * ratio\n\u001b[32m    226\u001b[39m policy_loss_2 = advantages * th.clamp(ratio, \u001b[32m1\u001b[39m - clip_range, \u001b[32m1\u001b[39m + clip_range)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m policy_loss = -\u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_loss_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_loss_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[32m    230\u001b[39m pg_losses.append(policy_loss.item())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TOTAL_TIMESTEPS = 200_000\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ec2a",
   "metadata": {},
   "source": [
    "## Save / load the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39cbd6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data/test_ppo_robot_arm_balance\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = MODELS_DIR / 'test_ppo_robot_arm_balance'\n",
    "model.save(MODEL_PATH)\n",
    "print('Model saved to', MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0019c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = MODELS_DIR / 'ppo_robot_arm_balance'\n",
    "model = PPO.load(MODEL_PATH, train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd70fe",
   "metadata": {},
   "source": [
    "## Rollout helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65319fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Iris(R) Xe Graphics (RPL-U)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 25.0.7-0ubuntu0.24.04.1\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 25.0.7-0ubuntu0.24.04.1\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Iris(R) Xe Graphics (RPL-U)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    }
   ],
   "source": [
    "train_env.close()\n",
    "\n",
    "def rollout_episode(env, model, max_steps=MAX_STEPS, deterministic=True):\n",
    "    obs, _ = env.reset()\n",
    "    rewards = []\n",
    "    infos = []\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = model.predict(obs, deterministic=deterministic)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return np.array(rewards), infos\n",
    "\n",
    "eval_env = Task7PendulumEnv(\n",
    "    max_steps=MAX_STEPS, should_balance=SHOULD_BALANCE, gui=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "782f6673",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rewards, infos = \u001b[43mrollout_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEpisode length:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rewards))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTotal reward:\u001b[39m\u001b[33m'\u001b[39m, rewards.sum())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrollout_episode\u001b[39m\u001b[34m(env, model, max_steps, deterministic)\u001b[39m\n\u001b[32m      6\u001b[39m infos = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     action, _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     obs, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     10\u001b[39m     rewards.append(reward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:557\u001b[39m, in \u001b[36mBaseAlgorithm.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    539\u001b[39m     observation: Union[np.ndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m     deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    543\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np.ndarray, ...]]]:\n\u001b[32m    544\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m \u001b[33;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:368\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    365\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[32m    370\u001b[39m actions = actions.cpu().numpy().reshape((-\u001b[32m1\u001b[39m, *\u001b[38;5;28mself\u001b[39m.action_space.shape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:717\u001b[39m, in \u001b[36mActorCriticPolicy._predict\u001b[39m\u001b[34m(self, observation, deterministic)\u001b[39m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m    710\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[33;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[32m    712\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Studies/MVA/Robotics/Project/.venv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:80\u001b[39m, in \u001b[36mDistribution.get_actions\u001b[39m\u001b[34m(self, deterministic)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmode\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> th.Tensor:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    Returns the most likely action (deterministic output)\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33;03m    from the probability distribution\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33;03m    :return: the stochastic action\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    Return actions according to the probability distribution.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03m    :param deterministic:\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    :return:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rewards, infos = rollout_episode(eval_env, model)\n",
    "print('Episode length:', len(rewards))\n",
    "print('Total reward:', rewards.sum())\n",
    "print('Final info:', infos[-1] if infos else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77f674",
   "metadata": {},
   "source": [
    "## Export rollouts for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63a97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_task7_dataset(\n",
    "    model,\n",
    "    n_episodes=5,\n",
    "    max_steps=MAX_STEPS,\n",
    "    should_balance=SHOULD_BALANCE,\n",
    "    filename='task7_ppo_dataset',\n",
    "):\n",
    "    env = Task7PendulumEnv(\n",
    "        max_steps=max_steps, should_balance=should_balance, gui=False\n",
    "    )\n",
    "    all_obs, all_actions, all_rewards, episode_ids = [], [], [], []\n",
    "    recorded = 0\n",
    "    while recorded < n_episodes:\n",
    "        obs, _ = env.reset()\n",
    "        local_obs, local_actions, local_rewards = [], [], []\n",
    "        success = True\n",
    "        for step in range(max_steps):\n",
    "            local_obs.append(obs.copy())\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            action = int(action)\n",
    "            local_actions.append(action)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            local_rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                success = info.get('success', False) and not info.get('failure', False)\n",
    "                break\n",
    "        if success and len(local_obs) == max_steps:\n",
    "            all_obs.extend(local_obs)\n",
    "            all_actions.extend(local_actions)\n",
    "            all_rewards.extend(local_rewards)\n",
    "            episode_ids.extend([recorded] * len(local_obs))\n",
    "            recorded += 1\n",
    "            print(f'Recorded episode {recorded}/{n_episodes}')\n",
    "        else:\n",
    "            print('Episode failed, retrying...')\n",
    "    env.close()\n",
    "    save_path = DATA_DIR / f'{filename}.npz'\n",
    "    np.savez(\n",
    "        save_path,\n",
    "        observations=np.array(all_obs, dtype=np.float32),\n",
    "        actions=np.array(all_actions, dtype=np.int64),\n",
    "        rewards=np.array(all_rewards, dtype=np.float32),\n",
    "        episode_ids=np.array(episode_ids, dtype=np.int32),\n",
    "        max_steps=max_steps,\n",
    "    )\n",
    "    print('Dataset saved to', save_path)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4ece55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded episode 1/3\n",
      "Recorded episode 2/3\n",
      "Recorded episode 3/3\n",
      "Dataset saved to data/task7_ppo_balance_rollouts.npz\n"
     ]
    }
   ],
   "source": [
    "# Example usage once a good policy is trained\n",
    "dataset_path = record_task7_dataset(model, n_episodes=3, filename='task7_ppo_balance_rollouts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4d2dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "Thread TERMINATED\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "eval_env.close()\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4045a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
